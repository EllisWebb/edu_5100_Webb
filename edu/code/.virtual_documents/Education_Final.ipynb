# Import pandas, numpy, and matplotlib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# seaborn is a data visualization library built on matplotlib
import seaborn as sns
# set the plotting style
sns.set_style("whitegrid")

#geo plotting
import plotly.offline as po
import plotly.graph_objs as pg

#iterative imputer tools
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Model preprocessing
from sklearn.preprocessing import StandardScaler

# Modeling
import statsmodels.formula.api as smf
import statsmodels.api as sm

# Model metrics and analysis
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from statsmodels.stats.anova import anova_lm


edgap = pd.read_excel(
    '/Users/carterwebb/Desktop/5100_Foundation_Data/edu_5100_Webb/edu/data/EdGap_data.xlsx',
    dtype={'NCESSCH School ID': object}
)


!curl -L -o ccd_sch_029_1617_w_1a_11212017.csv 'https://www.dropbox.com/s/lkl5nvcdmwyoban/ccd_sch_029_1617_w_1a_11212017.csv?dl=0'
edu_info = pd.read_csv(
    "ccd_sch_029_1617_w_1a_11212017.csv", encoding="unicode_escape"
)

pupil = pd.read_csv(
    '/Users/carterwebb/Desktop/5100_Foundation_Data/edu_5100_Webb/edu/data/exp_pupil.csv',
)

















edgap = edgap.rename(
    columns={
        "NCESSCH School ID": "id",
        "CT Pct Adults with College Degree": "percent_college",
        "CT Unemployment Rate": "rate_unemployment",
        "CT Pct Childre In Married Couple Family": "percent_married",
        "CT Median Household Income": "median_income",
        "School ACT average (or equivalent if SAT score)": "average_act",
        "School Pct Free and Reduced Lunch": "percent_lunch",
    }
)








edu_info = edu_info[
    ['SCHOOL_YEAR', 'NCESSCH', 'LEAID', 'LSTATE', 'LZIP', 'SCH_TYPE_TEXT', 'LEVEL', 'CHARTER_TEXT'
    ]
]


edu_info = edu_info.rename(
    columns={
        "SCHOOL_YEAR": "year",
        "NCESSCH": "id",
        "LEAID": "l_id",
        "LSTATE": "state",
        "LZIP": "zip_code",
        "SCH_TYPE_TEXT": "school_type",
        "LEVEL": "school_level",
        "CHARTER_TEXT": "charter"
    }
)





edu_info[['id', 'l_id']] = edu_info[['id', 'l_id']].astype('object')








pupil = pupil[
    [ "Agency ID - NCES Assigned [District] Latest available year", "Total Current Expenditures - Salary (Z32) per Pupil (V33) [District Finance] 2016-17"
    ]
]


pupil = pupil.rename(
    columns={
        "Agency ID - NCES Assigned [District] Latest available year": "l_id",
        "Total Current Expenditures - Salary (Z32) per Pupil (V33) [District Finance] 2016-17": "salary_pupil"
    }
)





pupil['salary_pupil'] = pd.to_numeric(pupil['salary_pupil'], errors='coerce')





pupil[['l_id']] = pupil[['l_id']].astype('object')








ed_merge = edgap.merge(
    edu_info,
    how='left',
    on='id'
)





edu = ed_merge.merge(
    pupil,
    how='left',
    on='l_id'
)





edu.select_dtypes(include=['number']).agg(['min', 'max']).round(2)





edu.loc[edu['percent_lunch'] < 0, 'percent_lunch'] = np.nan


edu.loc[edu['average_act'] < 1, 'average_act'] = np.nan


edu.loc[edu['salary_pupil'] == 0, 'salary_pupil'] = np.nan





edu = edu.loc[edu['school_level'] == 'High']





edu.isna().sum().to_frame(name='Number of Missing Values')





edu = edu.dropna(subset=['average_act'])





predictor_variables = [
    'rate_unemployment',
    'percent_college',
    'percent_married',
    'median_income',
    'percent_lunch',
    'state',
    'charter',
    'salary_pupil'
]

imputer = IterativeImputer()





numerical_predictors = edu[predictor_variables].select_dtypes(include='number').columns.to_list()

print(numerical_predictors)





imputer.fit(edu.loc[:, numerical_predictors])





edu.loc[:, numerical_predictors] = imputer.transform(edu.loc[:, numerical_predictors])


edu.isna().sum().to_frame(name='Number of Missing Values')


edu.to_csv(
    'education_clean.csv',
    encoding='utf-8-sig',
    index=False
)








layout = dict(
    geo={"scope": "usa"}, coloraxis_colorbar=dict(title="Number of Schools")
)

data = dict(
    type="choropleth",
    locations=edu["state"].value_counts().index,
    locationmode="USA-states",
    z=edu["state"].value_counts().values,
    coloraxis="coloraxis",
)

x = pg.Figure(data=[data], layout=layout)

po.iplot(x)


predictor_variables = ['rate_unemployment', 'percent_college', 'percent_married', 'median_income', 'percent_lunch', 'state', 'charter', 'salary_pupil']

numerical_predictors = edu[predictor_variables].select_dtypes(include='number').columns.to_list()

corr_matrix = edu[numerical_predictors + ["average_act"]].corr()

plt.figure(figsize=(10, 5))

sns.heatmap(
    corr_matrix, vmax=1, vmin=-1, square=True, annot=True, cmap="viridis"
)

plt.tick_params(labelsize=12)

plt.show()


numerical_predictors = ['rate_unemployment', 'percent_college', 'percent_married', 
                        'median_income', 'percent_lunch', 'salary_pupil']
corr = edu[numerical_predictors + ['average_act']].corr()

g = sns.pairplot(
    data=edu,
    vars=numerical_predictors + ['average_act'],
    kind="reg",
    plot_kws={"scatter_kws": {"alpha": 0.5, "color": "k", "s": 7}},
)

cmap = plt.get_cmap("viridis")


norm = plt.Normalize(vmin=-1, vmax=1)

for i, row_var in enumerate(corr.columns):
    for j, col_var in enumerate(corr.columns):
        ax = g.axes[i, j]
        if ax is not None:
            r = corr.loc[row_var, col_var]
            color = cmap(norm(r))
            ax.set_facecolor(color)

for ax in g.axes.flat:
    ax.set_xlabel(ax.get_xlabel(), fontsize=14, rotation=30, ha='right')
    ax.set_ylabel(ax.get_ylabel(), fontsize=14)
    plt.setp(ax.get_xticklabels(), rotation=30, ha='right')

plt.suptitle("Pairplot & Correlation Heatmap", y=1.02, fontsize=20, fontweight='bold')

sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
g.fig.colorbar(sm, ax=g.axes, shrink=0.6, label="Correlation")

plt.show()








mlr_full = smf.ols(
    formula='average_act ~ rate_unemployment + percent_college + percent_married + median_income + percent_lunch + salary_pupil',
    data=edu).fit()

print(mlr_full.summary())


mean_absolute_error(edu['average_act'], mlr_full.predict())





plt.figure(figsize=(6, 6))

sns.regplot(data=edu,
            x='median_income',
            y='average_act',
            color='blue',
            ci=False,
            scatter_kws={'color': 'black', 'edgecolors': 'white', 'linewidths': 1})

sns.regplot(data=edu,
            x='median_income',
            y='average_act',
            order=2, ## This is where you specify the quadratic model. 
            color='orange',
            ci=False,
            scatter=False)

plt.xlabel('Median income ($)', fontsize=16)
plt.ylabel('Average ACT score', fontsize=16)

plt.tick_params(labelsize=14)

plt.show()


plt.figure(figsize=(6, 6))

sns.regplot(data=edu,
            x='salary_pupil',
            y='average_act',
            color='blue',
            ci=False,
            scatter_kws={'color': 'black', 'edgecolors': 'white', 'linewidths': 1})

sns.regplot(data=edu,
            x='salary_pupil',
            y='average_act',
            order=2, ## This is where you specify the quadratic model. 
            color='orange',
            ci=False,
            scatter=False)

# Add axis labels
plt.xlabel('Salary Per Pupil ($)', fontsize=16)
plt.ylabel('Average ACT score', fontsize=16)

# Increase the fontsize of the tick labels
plt.tick_params(labelsize=14)

plt.show()





mlr_reduced = smf.ols(
    formula='average_act ~ rate_unemployment + percent_college + percent_lunch + salary_pupil',
    data=edu).fit()

print(mlr_reduced.summary())





mae_full = mean_absolute_error(edu['average_act'], mlr_full.predict())
mae_reduced = mean_absolute_error(edu['average_act'], mlr_reduced.predict())

r2_full = mlr_full.rsquared
r2_reduced = mlr_reduced.rsquared

pd.DataFrame({'Mean Absolute Error': [mae_full, mae_reduced],
              'R-squared': [r2_full, r2_reduced]},
              index=['full model', 'reduced model']).round(4)





norm_var = ['rate_unemployment', 'percent_college', 'percent_lunch', 'salary_pupil']

scaled_columns = [var + '_normalized' for var in norm_var]

scaler = StandardScaler().fit(edu[norm_var])

edu[scaled_columns] = scaler.transform(edu[norm_var])


edu[scaled_columns].agg(['mean', 'std']).round(3)





mlr_norm = smf.ols(
    formula='average_act ~ rate_unemployment_normalized + percent_college_normalized + percent_lunch_normalized + salary_pupil_normalized',
    data=edu).fit()


print(mlr_norm.summary())





lr = smf.ols(
    formula='average_act ~ percent_lunch_normalized',
    data=edu).fit()


print(lr.summary())


mae_full = mean_absolute_error(edu['average_act'], mlr_full.predict())
mae_reduced = mean_absolute_error(edu['average_act'], mlr_reduced.predict())
mae_norm = mean_absolute_error(edu['average_act'], mlr_norm.predict())
mae_single = mean_absolute_error(edu['average_act'], lr.predict())

r2_full = mlr_full.rsquared
r2_reduced = mlr_reduced.rsquared
r2_norm = mlr_full.rsquared
r2_single = lr.rsquared

pd.DataFrame({'Mean Absolute Error': [mae_full, mae_reduced, mae_norm, mae_single],
              'R-squared': [r2_full, r2_reduced, r2_norm, r2_single]},
              index=['full model', 'reduced model', 'normalized model', '% lunch only']).round(4)



